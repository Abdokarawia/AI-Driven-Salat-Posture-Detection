{"cells":[{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6432,"status":"ok","timestamp":1723567146317,"user":{"displayName":"تفقد","userId":"04243839035669783599"},"user_tz":-180},"id":"Z-bjHa0BZzG6","outputId":"903936a4-4279-4480-f869-c49c38ae1fa2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyngrok\n","  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n","Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-7.2.0\n"]}],"source":["!pip install pyngrok"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":70029,"status":"ok","timestamp":1723567078479,"user":{"displayName":"تفقد","userId":"04243839035669783599"},"user_tz":-180},"id":"sfHeM2yUMfic","outputId":"95d682ac-c350-42a1-8677-643e38f80a35"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.2.76-py3-none-any.whl.metadata (41 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.26.4)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (3.7.1)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.10.0.84)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.4.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (1.13.1)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.3.1+cu121)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.18.1+cu121)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (4.66.5)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (2.1.4)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics) (0.13.1)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.0-py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n","Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n","Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n","Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->ultralytics) (2.3.1)\n","Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.8.0->ultralytics)\n","  Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n","Downloading ultralytics-8.2.76-py3-none-any.whl (865 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.6/865.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hUsing cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n","Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n","Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n","Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n","Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n","Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n","Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n","Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n","Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n","Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n","Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n","Downloading ultralytics_thop-2.0.0-py3-none-any.whl (25 kB)\n","Using cached nvidia_nvjitlink_cu12-12.6.20-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n","Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.20 nvidia-nvtx-cu12-12.1.105 ultralytics-8.2.76 ultralytics-thop-2.0.0\n"]}],"source":["!pip install ultralytics"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20860,"status":"ok","timestamp":1723567122093,"user":{"displayName":"تفقد","userId":"04243839035669783599"},"user_tz":-180},"id":"afkGjqlCSbn0","outputId":"65a136b6-fe0f-4e0c-c999-59ec6f9f39c2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting mediapipe\n","  Downloading mediapipe-0.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.7 kB)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.4.0)\n","Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.2.0)\n","Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from mediapipe) (24.3.25)\n","Requirement already satisfied: jax in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26)\n","Requirement already satisfied: jaxlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (0.4.26+cuda12.cudnn89)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from mediapipe) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from mediapipe) (1.26.4)\n","Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.10/dist-packages (from mediapipe) (4.10.0.84)\n","Collecting protobuf<5,>=4.25.3 (from mediapipe)\n","  Downloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n","Collecting sounddevice>=0.4.4 (from mediapipe)\n","  Downloading sounddevice-0.5.0-py3-none-any.whl.metadata (1.4 kB)\n","Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.10/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.0)\n","Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (0.4.0)\n","Requirement already satisfied: opt-einsum in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (3.3.0)\n","Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.10/dist-packages (from jax->mediapipe) (1.13.1)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.2.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (4.53.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (1.4.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (24.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (3.1.2)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->mediapipe) (2.8.2)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n","Downloading mediapipe-0.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.7/35.7 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading protobuf-4.25.4-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading sounddevice-0.5.0-py3-none-any.whl (32 kB)\n","Installing collected packages: protobuf, sounddevice, mediapipe\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.20.3\n","    Uninstalling protobuf-3.20.3:\n","      Successfully uninstalled protobuf-3.20.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 4.25.4 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed mediapipe-0.10.14 protobuf-4.25.4 sounddevice-0.5.0\n"]}],"source":["!pip install mediapipe"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22776,"status":"ok","timestamp":1723567101243,"user":{"displayName":"تفقد","userId":"04243839035669783599"},"user_tz":-180},"id":"6Euth2PDLJfK","outputId":"33c2d76c-218a-4c28-8253-56aa588934c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Importing the drive module from Google Colab to mount Google Drive (Authorization).\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"YB5kgWG5Kq0e","executionInfo":{"status":"ok","timestamp":1723567503686,"user_tz":-180,"elapsed":401,"user":{"displayName":"تفقد","userId":"04243839035669783599"}}},"outputs":[],"source":["from flask import Flask, request, render_template, jsonify  # Import Flask and other modules for web application\n","from flask import Flask, send_file  # Import Flask and send_file for serving images\n","from io import BytesIO  # Import BytesIO for handling binary data\n","import matplotlib.pyplot as plt  # Import matplotlib for plotting images\n","from PIL import Image  # Import PIL for image processing\n","import matplotlib.image as mpimg  # Import matplotlib.image for reading images\n","from ultralytics import YOLO # YOLO object detection model\n","import mediapipe as mp # Mediapipe for pose estimation\n","import cv2 # OpenCV for image processing\n","import os # For file operations\n","import random # For random number generation\n","from matplotlib.patches import Rectangle  # For drawing rectangles on images\n","\n","import math  # For mathematical operations\n","import io  # For handling binary data\n","import base64  # For encoding/decoding binary data\n","import numpy as np  # For numerical operations\n","from operator import imod  # For modulo operation\n","\n","from pyngrok import ngrok  # For exposing local server to the internet\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":366,"status":"ok","timestamp":1723567505930,"user":{"displayName":"تفقد","userId":"04243839035669783599"},"user_tz":-180},"id":"_t7tXD5_Kq0j"},"outputs":[],"source":["# Load a model\n","model = YOLO('yolov8n.pt')  # load an official model\n","\n","model = YOLO(\"/content/yolov8n.pt\")  # load a custom model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bRtlarzfKq0k","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cb5f916b-9ae3-43d6-d930-4a1afcaccff6"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["https://242f-35-227-136-47.ngrok-free.app\n"," * Serving Flask app '__main__'\n"," * Debug mode: off\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n"," * Running on http://127.0.0.1:5000\n","INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","image 1/1 /content/Screenshotsruku-1.jpg: 640x448 1 ruku, 81.8ms\n","Speed: 12.1ms preprocess, 81.8ms inference, 1243.1ms postprocess per image at shape (1, 3, 640, 448)\n","Bounding box coordinates (x1, y1, x2, y2): 132.9288330078125 416.2718505859375 725.060302734375 1186.594482421875\n","The predicted class is (132, 416, 725, 1186)\n","\n","image 1/1 /content/Screenshotsruku-1.jpg: 640x448 1 ruku, 20.6ms\n","Speed: 3.7ms preprocess, 20.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 448)\n","ultralytics.engine.results.Boxes object with attributes:\n","\n","cls: tensor([1.], device='cuda:0')\n","conf: tensor([0.7949], device='cuda:0')\n","data: tensor([[1.3293e+02, 4.1627e+02, 7.2506e+02, 1.1866e+03, 7.9489e-01, 1.0000e+00]], device='cuda:0')\n","id: None\n","is_track: False\n","orig_shape: (1280, 891)\n","shape: torch.Size([1, 6])\n","xywh: tensor([[428.9946, 801.4332, 592.1315, 770.3226]], device='cuda:0')\n","xywhn: tensor([[0.4815, 0.6261, 0.6646, 0.6018]], device='cuda:0')\n","xyxy: tensor([[ 132.9288,  416.2719,  725.0603, 1186.5945]], device='cuda:0')\n","xyxyn: tensor([[0.1492, 0.3252, 0.8138, 0.9270]], device='cuda:0')\n","Bounding box coordinates (x1, y1, x2, y2): 132.9288330078125 416.2718505859375 725.060302734375 1186.594482421875\n","0.7221162796020508\n","0.013415515422821045\n","-0.034637391567230225\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n","  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n","INFO:werkzeug:127.0.0.1 - - [13/Aug/2024 16:46:20] \"POST /Raising HTTP/1.1\" 200 -\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","image 1/1 /content/takbeer-1.jpg: 448x640 1 takbeer, 123.6ms\n","Speed: 9.3ms preprocess, 123.6ms inference, 2.2ms postprocess per image at shape (1, 3, 448, 640)\n","Bounding box coordinates (x1, y1, x2, y2): 99.73641204833984 0.0 478.6961975097656 301.232666015625\n","The predicted class is (99, 0, 478, 301)\n","\n","image 1/1 /content/takbeer-1.jpg: 448x640 1 takbeer, 20.8ms\n","Speed: 6.8ms preprocess, 20.8ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n","ultralytics.engine.results.Boxes object with attributes:\n","\n","cls: tensor([2.], device='cuda:0')\n","conf: tensor([0.9275], device='cuda:0')\n","data: tensor([[ 99.7364,   0.0000, 478.6962, 301.2327,   0.9275,   2.0000]], device='cuda:0')\n","id: None\n","is_track: False\n","orig_shape: (360, 540)\n","shape: torch.Size([1, 6])\n","xywh: tensor([[289.2163, 150.6163, 378.9598, 301.2327]], device='cuda:0')\n","xywhn: tensor([[0.5356, 0.4184, 0.7018, 0.8368]], device='cuda:0')\n","xyxy: tensor([[ 99.7364,   0.0000, 478.6962, 301.2327]], device='cuda:0')\n","xyxyn: tensor([[0.1847, 0.0000, 0.8865, 0.8368]], device='cuda:0')\n","Bounding box coordinates (x1, y1, x2, y2): 99.73641204833984 0.0 478.6961975097656 301.232666015625\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n","  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["1.6910650513388894\n","1.4881372451782227\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [13/Aug/2024 16:49:03] \"POST /Takbeer HTTP/1.1\" 200 -\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["\n","image 1/1 /content/raising-1.jpg: 448x640 1 raising, 67.8ms\n","Speed: 3.3ms preprocess, 67.8ms inference, 1.5ms postprocess per image at shape (1, 3, 448, 640)\n","Bounding box coordinates (x1, y1, x2, y2): 1514.62548828125 436.95147705078125 3415.0595703125 3258.8203125\n","The predicted class is (1514, 436, 3415, 3258)\n","\n","image 1/1 /content/raising-1.jpg: 448x640 1 raising, 12.9ms\n","Speed: 5.9ms preprocess, 12.9ms inference, 1.9ms postprocess per image at shape (1, 3, 448, 640)\n","ultralytics.engine.results.Boxes object with attributes:\n","\n","cls: tensor([0.], device='cuda:0')\n","conf: tensor([0.9377], device='cuda:0')\n","data: tensor([[1.5146e+03, 4.3695e+02, 3.4151e+03, 3.2588e+03, 9.3769e-01, 0.0000e+00]], device='cuda:0')\n","id: None\n","is_track: False\n","orig_shape: (3264, 4928)\n","shape: torch.Size([1, 6])\n","xywh: tensor([[2464.8425, 1847.8859, 1900.4341, 2821.8689]], device='cuda:0')\n","xywhn: tensor([[0.5002, 0.5661, 0.3856, 0.8645]], device='cuda:0')\n","xyxy: tensor([[1514.6255,  436.9515, 3415.0596, 3258.8203]], device='cuda:0')\n","xyxyn: tensor([[0.3074, 0.1339, 0.6930, 0.9984]], device='cuda:0')\n","Bounding box coordinates (x1, y1, x2, y2): 1514.62548828125 436.95147705078125 3415.0595703125 3258.8203125\n","2.1116757869720457\n","0.07629016041755676\n","0.006501734256744385\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["INFO:werkzeug:127.0.0.1 - - [13/Aug/2024 16:53:18] \"POST /Raising HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/Raising.jpg: 448x640 1 raising, 71.1ms\n","Speed: 4.1ms preprocess, 71.1ms inference, 1.6ms postprocess per image at shape (1, 3, 448, 640)\n","Bounding box coordinates (x1, y1, x2, y2): 1514.62548828125 436.95147705078125 3415.0595703125 3258.8203125\n","The predicted class is (1514, 436, 3415, 3258)\n","\n","image 1/1 /content/Raising.jpg: 448x640 1 raising, 12.7ms\n","Speed: 2.9ms preprocess, 12.7ms inference, 1.3ms postprocess per image at shape (1, 3, 448, 640)\n","ultralytics.engine.results.Boxes object with attributes:\n","\n","cls: tensor([0.], device='cuda:0')\n","conf: tensor([0.9377], device='cuda:0')\n","data: tensor([[1.5146e+03, 4.3695e+02, 3.4151e+03, 3.2588e+03, 9.3769e-01, 0.0000e+00]], device='cuda:0')\n","id: None\n","is_track: False\n","orig_shape: (3264, 4928)\n","shape: torch.Size([1, 6])\n","xywh: tensor([[2464.8425, 1847.8859, 1900.4341, 2821.8689]], device='cuda:0')\n","xywhn: tensor([[0.5002, 0.5661, 0.3856, 0.8645]], device='cuda:0')\n","xyxy: tensor([[1514.6255,  436.9515, 3415.0596, 3258.8203]], device='cuda:0')\n","xyxyn: tensor([[0.3074, 0.1339, 0.6930, 0.9984]], device='cuda:0')\n","Bounding box coordinates (x1, y1, x2, y2): 1514.62548828125 436.95147705078125 3415.0595703125 3258.8203125\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n","  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"]},{"output_type":"stream","name":"stdout","text":["2.1116757869720457\n","0.07629016041755676\n","0.006501734256744385\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [13/Aug/2024 17:04:53] \"POST /Raising HTTP/1.1\" 200 -\n"]},{"output_type":"stream","name":"stdout","text":["\n","image 1/1 /content/raising-1 (3).jpg: 640x640 1 raising, 107.0ms\n","Speed: 5.4ms preprocess, 107.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","Bounding box coordinates (x1, y1, x2, y2): 547.4047241210938 151.12722778320312 2380.06787109375 3023.0\n","The predicted class is (547, 151, 2380, 3023)\n","\n","image 1/1 /content/raising-1 (3).jpg: 640x640 1 raising, 29.5ms\n","Speed: 5.4ms preprocess, 29.5ms inference, 4.1ms postprocess per image at shape (1, 3, 640, 640)\n","ultralytics.engine.results.Boxes object with attributes:\n","\n","cls: tensor([0.], device='cuda:0')\n","conf: tensor([0.9254], device='cuda:0')\n","data: tensor([[5.4740e+02, 1.5113e+02, 2.3801e+03, 3.0230e+03, 9.2537e-01, 0.0000e+00]], device='cuda:0')\n","id: None\n","is_track: False\n","orig_shape: (3023, 2885)\n","shape: torch.Size([1, 6])\n","xywh: tensor([[1463.7363, 1587.0636, 1832.6631, 2871.8728]], device='cuda:0')\n","xywhn: tensor([[0.5074, 0.5250, 0.6352, 0.9500]], device='cuda:0')\n","xyxy: tensor([[ 547.4047,  151.1272, 2380.0679, 3023.0000]], device='cuda:0')\n","xyxyn: tensor([[0.1897, 0.0500, 0.8250, 1.0000]], device='cuda:0')\n","Bounding box coordinates (x1, y1, x2, y2): 547.4047241210938 151.12722778320312 2380.06787109375 3023.0\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n","  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"]},{"output_type":"stream","name":"stdout","text":["3.6410034179687503\n","0.11455577611923218\n","0.006469309329986572\n"]},{"output_type":"stream","name":"stderr","text":["INFO:werkzeug:127.0.0.1 - - [13/Aug/2024 17:06:13] \"POST /Raising HTTP/1.1\" 200 -\n"]}],"source":["app = Flask(__name__)\n","ngrok.set_auth_token(\"2gTWkFi5q5FaBt0VyACLQCPfCwT_3k1vzLFMqsfaHBMXs5zcT\")\n","public_url =  ngrok.connect(5000).public_url\n","print(public_url)\n","model = YOLO(\"/content/drive/MyDrive/TFQD API/best.pt\")  # load a custom model\n","\n","@app.route('/Ruku', methods=['POST', 'GET'])\n","def Detect_Ruku():\n","    imagefile = request.files['imagefile']\n","    image_path = imagefile.filename\n","    imagefile.save(image_path)\n","\n","    results = model.predict(image_path)\n","\n","\n","    for r in results:\n","        # Extract bounding box coordinates\n","        bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n","        print(\"Bounding box coordinates (x1, y1, x2, y2):\", x1, y1, x2, y2)\n","        bounding_box_tuple = tuple(bounding_box)\n","\n","\n","    bounding_box_int = tuple(int(value) for value in bounding_box_tuple)\n","\n","    print('The predicted class is ' + str(bounding_box_int))\n","\n","\n","    # Initialize Mediapipe Pose model\n","    mp_pose = mp.solutions.pose\n","    pose = mp_pose.Pose()\n","\n","\n","    # Function to calculate angle between lines formed by three points\n","    def calculate_angle(a, b, c):\n","        angle_rad1 = math.atan2(c[1]-b[1], c[0]-b[0]) - math.atan2(a[1]-b[1], a[0]-b[0])\n","        angle_rad1 = angle_rad1 % (2 * math.pi)\n","        angle_rad2 = math.atan2(a[1]-b[1], a[0]-b[0]) - math.atan2(c[1]-b[1], c[0]-b[0])\n","        angle_rad2 = angle_rad2 % (2 * math.pi)\n","        return math.degrees(min(angle_rad1, angle_rad2))\n","\n","    # Function to check if the pose resembles the ruku position in Salat\n","    def is_ruku_pose(landmarks):\n","        left_hip = (landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].x, landmarks[mp_pose.PoseLandmark.LEFT_HIP.value].y)\n","        right_hip = (landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value].y)\n","        left_shoulder = (landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y)\n","        right_shoulder = (landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y)\n","        left_knee = (landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.LEFT_KNEE.value].y)\n","        right_knee = (landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].x, landmarks[mp_pose.PoseLandmark.RIGHT_KNEE.value].y)\n","\n","        angle_left = abs(90 - calculate_angle(left_knee, left_hip, left_shoulder))\n","        angle_right = abs(90 - calculate_angle(right_knee, right_hip, right_shoulder))\n","\n","        ruku_angle_threshold = 30  # Adjust as needed\n","        if angle_left <= ruku_angle_threshold or angle_right <= ruku_angle_threshold:\n","            return True, min(abs(angle_left), abs(angle_right)) / 100 * 100 # Calculate error percentage\n","        else:\n","            return False, min(abs(angle_left), abs(angle_right)) / 100 * 100\n","\n","    def process_image(image_path):\n","        results = model.predict(image_path)\n","\n","        for r in results:\n","            bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n","\n","        bounding_box = tuple(int(value) for value in bounding_box)\n","\n","        image = cv2.imread(image_path)\n","        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        results = pose.process(image_rgb)\n","        is_ruku, error_percentage = is_ruku_pose(results.pose_landmarks.landmark)\n","\n","\n","        # Red color if error>15, else green\n","\n","        if error_percentage<= 15:\n","          color = (0, 255, 0)\n","        else:\n","          color = (255, 0, 0)\n","\n","        # Line thickness of 3 px\n","        thickness = 3\n","\n","            # Draw bounding box\n","        x1, y1, x2, y2 = bounding_box\n","        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, thickness)\n","\n","\n","        Perfect_Threshold = 5;\n","\n","\n","        plt.figure(figsize=(8, 8))\n","        plt.imshow(image_rgb)\n","        mp_drawing = mp.solutions.drawing_utils\n","        mp_drawing.draw_landmarks(image_rgb, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n","\n","        for landmark in [mp_pose.PoseLandmark.LEFT_KNEE, mp_pose.PoseLandmark.RIGHT_KNEE,\n","                        mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP,\n","                        mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER]:\n","            plt.scatter(results.pose_landmarks.landmark[landmark.value].x * image.shape[1],\n","                        results.pose_landmarks.landmark[landmark.value].y * image.shape[0],\n","                        color='white', s=100)\n","\n","        plt.plot([results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP.value].x * image.shape[1],\n","                results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE.value].x * image.shape[1]],\n","                [results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP.value].y * image.shape[0],\n","                results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_KNEE.value].y * image.shape[0]],\n","                color='cyan', linestyle='-', linewidth=2, alpha=0.5)\n","        plt.plot([results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP.value].x * image.shape[1],\n","                results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE.value].x * image.shape[1]],\n","                [results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP.value].y * image.shape[0],\n","                results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_KNEE.value].y * image.shape[0]],\n","                color='cyan', linestyle='-', linewidth=2, alpha=0.5)\n","\n","        plt.plot([results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP.value].x * image.shape[1],\n","                results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER.value].x * image.shape[1]],\n","                [results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_HIP.value].y * image.shape[0],\n","                results.pose_landmarks.landmark[mp_pose.PoseLandmark.LEFT_SHOULDER.value].y * image.shape[0]],\n","                color='magenta', linestyle='-', linewidth=2, alpha=0.5)\n","        plt.plot([results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP.value].x * image.shape[1],\n","                results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].x * image.shape[1]],\n","                [results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_HIP.value].y * image.shape[0],\n","                results.pose_landmarks.landmark[mp_pose.PoseLandmark.RIGHT_SHOULDER.value].y * image.shape[0]],\n","                color='magenta', linestyle='-', linewidth=3, alpha=0.5)\n","\n","        print('Done')\n","        plt.axis('off')\n","        buffer = io.BytesIO()\n","        plt.savefig(buffer, format='png')\n","        buffer.seek(0)\n","        image_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n","        plt.close()\n","        return image_base64, error_percentage, is_ruku\n","\n","\n","\n","    # Process the image and get the base64 encoded plot\n","    plot_base64, error, is_ruku= process_image(image_path)\n","    error_text = \"{:.2f}\".format(error)\n","\n","    if is_ruku and error <= 5:\n","        return jsonify({'image_base64': plot_base64, 'error_description': \"وضعية الصلاة بالصوره المرفوعه وضعية ركوع مثالي\"})\n","    if is_ruku and error>5 and error<=15:\n","        return jsonify({'image_base64': plot_base64, 'error_description': \" ركوع صحيح لكن فيه نسبة انحراف عن الأصل بمقدار: {} % للتأكد من الصفة الصحيحة يمكنك مراجعة ارشاداتنا\".format(error_text)})\n","    if is_ruku and error>15 and error<=30:\n","        return jsonify({'image_base64': plot_base64, 'error_description': \" ركوع صحيح لكن فيه نسبة انحراف كبيرة عن الأصل بمقدار:{} % للتأكد من الصفة الصحيحة يمكنك مراجعة ارشاداتنا\".format(error_text)})\n","    else:\n","        return jsonify({'image_base64': plot_base64, 'error_description': \"وضعية الصلاه بالصوره المرفوعه ليست وضعية ركوع\"})\n","\n","\n","@app.route('/Takbeer', methods=['POST', 'GET'])\n","def Detect_Takbeer():\n","    imagefile = request.files['imagefile']\n","    image_path =  imagefile.filename\n","    imagefile.save(image_path)\n","\n","    results = model.predict(image_path)\n","\n","\n","    for r in results:\n","        # Extract bounding box coordinates\n","        bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n","        print(\"Bounding box coordinates (x1, y1, x2, y2):\", x1, y1, x2, y2)\n","        bounding_box_tuple = tuple(bounding_box)\n","\n","\n","    bounding_box_int = tuple(int(value) for value in bounding_box_tuple)\n","\n","    print('The predicted class is ' + str(bounding_box_int))\n","\n","\n","    # Initialize Mediapipe Pose model\n","    mp_pose = mp.solutions.pose\n","    pose = mp_pose.Pose()\n","\n","    # Function to calculate angle between lines formed by three points\n","    def calculate_angle(a, b, c):\n","        angle_rad = math.atan2(c[1]-b[1], c[0]-b[0]) - math.atan2(a[1]-b[1], a[0]-b[0])\n","        angle_rad = angle_rad % (2 * math.pi)\n","        return math.degrees(angle_rad)\n","\n","    # Function to check if the pose resembles the Takbeer position in Salat\n","    def is_takbeer_pose(landmarks):\n","        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n","        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n","        left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n","        right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n","        left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n","        right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n","        left_index = landmarks[mp_pose.PoseLandmark.LEFT_INDEX.value]\n","        right_index = landmarks[mp_pose.PoseLandmark.RIGHT_INDEX.value]\n","\n","        print(abs(abs(left_wrist.y)-abs(right_wrist.y)) / 0.22 * 100)\n","\n","        Perfect_Threshold = 5;\n","        # Check if left and right wrist are on the same line\n","        if abs(left_wrist.y - right_wrist.y) < 0.05 and abs(left_wrist.x - right_wrist.x) > 0.15:\n","            # Check if distance between right hip and right shoulder in y-direction is big enough\n","            if right_hip.y - right_shoulder.y > 0.25 or left_hip.y - left_shoulder.y > 0.25 :\n","              return True, abs(abs(left_wrist.y)-abs(right_wrist.y)) / 0.25 * 100\n","        return False, abs(abs(left_wrist.y)-abs(right_wrist.y)) / 0.25 * 100\n","\n","    def process_image(image_path):\n","        model_results = model.predict(image_path)\n","\n","        for r in model_results:\n","            print(r.boxes)\n","            # Extract bounding box coordinates\n","            bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n","            print(\"Bounding box coordinates (x1, y1, x2, y2):\", x1, y1, x2, y2)\n","            bounding_box_tuple = tuple(bounding_box)\n","\n","        bounding_box = tuple(int(value) for value in bounding_box_tuple)\n","\n","        image = cv2.imread(image_path)\n","        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        # Detect pose landmarks\n","        results = pose.process(image_rgb)\n","        is_takbeer, error = is_takbeer_pose(results.pose_landmarks.landmark)\n","        print(error)\n","\n","        # Red color if error>15, else green\n","\n","        if error<= 15 and is_takbeer:\n","          color = (0, 255, 0)\n","        else:\n","          color = (255, 0, 0)\n","\n","        # Line thickness of 3 px\n","        thickness = 3\n","\n","\n","        # Load image\n","        # Draw bounding box\n","        x1, y1, x2, y2 = bounding_box\n","        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, thickness)\n","\n","        # Convert image to RGB\n","\n","        Perfect_Threshold = 5;\n","\n","\n","\n","        # Draw text above bounding box with background color\n","        # Draw text above bounding box with background color\n","        plt.figure(figsize=(8, 8))\n","        plt.imshow(image_rgb)\n","\n","        # Draw connections between all specified landmarks\n","        landmarks = results.pose_landmarks.landmark\n","        connections = [(mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER),\n","                      (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP),\n","                      (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP),\n","                      (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP),\n","                      (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_WRIST),\n","                      (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_WRIST),\n","                      (mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.LEFT_INDEX),\n","                      (mp_pose.PoseLandmark.RIGHT_WRIST, mp_pose.PoseLandmark.RIGHT_INDEX)]\n","\n","        # Draw points for knees, hips, and shoulders\n","        for landmark in [mp_pose.PoseLandmark.LEFT_INDEX, mp_pose.PoseLandmark.RIGHT_INDEX,\n","                        mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.RIGHT_WRIST,\n","                            mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP,\n","                            mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER]:\n","                plt.scatter(results.pose_landmarks.landmark[landmark.value].x * image.shape[1],\n","                            results.pose_landmarks.landmark[landmark.value].y * image.shape[0],\n","                            color='white', s=100)\n","\n","        for connection in connections:\n","            start_point = connection[0]\n","            end_point = connection[1]\n","            plt.plot([landmarks[start_point.value].x * image.shape[1], landmarks[end_point.value].x * image.shape[1]],\n","                    [landmarks[start_point.value].y * image.shape[0], landmarks[end_point.value].y * image.shape[0]],\n","                    color='magenta', linestyle='-', linewidth=1)\n","        # Draw landmarks and connections\n","        mp_drawing = mp.solutions.drawing_utils\n","        mp_drawing.draw_landmarks(image_rgb, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n","                                  landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=5),\n","                                  connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2))\n","        plt.axis('off')\n","        # Save the plot as bytes in memory\n","        buffer = io.BytesIO()\n","        plt.savefig(buffer, format='png')\n","        buffer.seek(0)\n","        # Convert the plot bytes to base64 string\n","        plot_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n","        plt.close()  # Close the plot to free memory\n","\n","        return plot_base64, error, is_takbeer\n","\n","    # Process the image and get the base64 encoded plot\n","    plot_base64,error, is_takbeer = process_image(image_path)\n","\n","    error_text = \"{:.2f}\".format(error)\n","\n","\n","    if is_takbeer and error <= 5:\n","        return jsonify({'image_base64': plot_base64, 'error_description': \"وضعية الصلاة بالصوره المرفوعه وضعية تكبير مثاليه\"})\n","    if is_takbeer and error>5 and error<=15:\n","        return jsonify({'image_base64': plot_base64, 'error_description': \" تكبير صحيح لكن فيه نسبة انحراف عن الأصل بمقدار:{} % للتأكد من الصفة الصحيحة يمكنك مراجعة ارشاداتنا\".format(error_text)})\n","    if is_takbeer and error>15 and error<=30:\n","        return jsonify({'image_base64': plot_base64, 'error_description': \" تكبير صحيح لكن فيه نسبة انحراف كبيرة عن الأصل بمقدار:{} % للتأكد من الصفة الصحيحة يمكنك مراجعة ارشاداتنا\".format(error_text)})\n","    else:\n","        return jsonify({'image_base64': plot_base64, 'error_description': \"وضعية الصلاه بالصوره المرفوعه ليست وضعية تكبير\"})\n","\n","\n","\n","@app.route('/Raising', methods=['POST', 'GET'])\n","def Detect_Raising():\n","    imagefile = request.files['imagefile']\n","    image_path = imagefile.filename\n","    imagefile.save(image_path)\n","\n","    results = model.predict(image_path)\n","\n","\n","    for r in results:\n","        # Extract bounding box coordinates\n","        bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n","        print(\"Bounding box coordinates (x1, y1, x2, y2):\", x1, y1, x2, y2)\n","        bounding_box_tuple = tuple(bounding_box)\n","\n","\n","    bounding_box_int = tuple(int(value) for value in bounding_box_tuple)\n","\n","    print('The predicted class is ' + str(bounding_box_int))\n","\n","\n","    # Initialize Mediapipe Pose model\n","    mp_pose = mp.solutions.pose\n","    pose = mp_pose.Pose()\n","\n","    def is_raising_pose(landmarks):\n","        # Extract relevant landmarks\n","        left_shoulder = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n","        right_shoulder = landmarks[mp_pose.PoseLandmark.RIGHT_SHOULDER.value]\n","        left_elbow = landmarks[mp_pose.PoseLandmark.LEFT_ELBOW.value]\n","        right_elbow = landmarks[mp_pose.PoseLandmark.RIGHT_ELBOW.value]\n","        left_hip = landmarks[mp_pose.PoseLandmark.LEFT_HIP.value]\n","        right_hip = landmarks[mp_pose.PoseLandmark.RIGHT_HIP.value]\n","        nose = landmarks[mp_pose.PoseLandmark.NOSE.value]\n","        chin = landmarks[mp_pose.PoseLandmark.LEFT_SHOULDER.value]\n","        left_wrist = landmarks[mp_pose.PoseLandmark.LEFT_WRIST.value]\n","        right_wrist = landmarks[mp_pose.PoseLandmark.RIGHT_WRIST.value]\n","\n","        print(abs(abs(abs(left_wrist.y)-abs(right_wrist.y))+abs(abs(left_wrist.x)-abs(right_wrist.x))-0.03)/ 2.5 * 100)\n","        print(left_wrist.x - right_wrist.x)\n","        print(left_wrist.y - right_wrist.y)\n","\n","        # Check if left and right wrist are close to each other\n","        if abs(left_wrist.x - right_wrist.x) and abs(left_wrist.y - right_wrist.y) < 0.05:\n","            # Check if distance between right hip and right shoulder in y-direction is big enough\n","            if right_hip.y - right_shoulder.y > 0.25 or left_hip.y - left_shoulder.y > 0.25 :\n","                if right_wrist.y - right_shoulder.y > 0.15 or left_wrist.y - left_shoulder.y > 0.15:\n","                    # Check  wrists are above the shoulders\n","                    return True, abs(abs(abs(left_wrist.y)-abs(right_wrist.y))+abs(abs(left_wrist.x)-abs(right_wrist.x))-0.03)/ 2.5 * 100\n","        return False, abs(abs(abs(left_wrist.y)-abs(right_wrist.y))+abs(abs(left_wrist.x)-abs(right_wrist.x))-0.03)/ 2.5 * 100\n","\n","    def process_image(image_path):\n","        # Load Model\n","        results = model.predict(image_path)\n","\n","        for r in results:\n","            print(r.boxes)\n","            # Extract bounding box coordinates\n","            bounding_box = x1, y1, x2, y2 = r.boxes.xyxy[0].tolist()\n","            print(\"Bounding box coordinates (x1, y1, x2, y2):\", x1, y1, x2, y2)\n","            bounding_box_tuple = tuple(bounding_box)\n","\n","        bounding_box = tuple(int(value) for value in bounding_box_tuple)\n","\n","        # Load image\n","        image = cv2.imread(image_path)\n","        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        # Detect pose landmarks\n","        results = pose.process(image_rgb)\n","        is_raising, error = is_raising_pose(results.pose_landmarks.landmark)\n","\n","\n","\n","\n","        # Red color if error>15, else green\n","\n","        if error<= 15 and is_raising:\n","          color = (0, 255, 0)\n","        else:\n","          color = (255, 0, 0)\n","\n","        # Line thickness of 3 px\n","        thickness = 3\n","\n","\n","        # Draw bounding box\n","        x1, y1, x2, y2 = bounding_box\n","        cv2.rectangle(image_rgb, (x1, y1), (x2, y2), color, thickness)\n","\n","        # Convert image to RGB\n","\n","        Perfect_Threshold = 5;\n","\n","\n","\n","        # Check if pose resembles raising pose\n","\n","        # Draw text above bounding box with background color\n","        plt.figure(figsize=(8, 8))\n","        plt.savefig(image_path)  # Specify the file name and extension\n","        plt.imshow(image_rgb)\n","\n","        mp_drawing = mp.solutions.drawing_utils\n","        mp_drawing.draw_landmarks(image_rgb, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n","\n","        # Draw connections between all specified landmarks\n","        landmarks = results.pose_landmarks.landmark\n","        connections = [(mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER),\n","                    (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_HIP),\n","                    (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_HIP),\n","                    (mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP),\n","                    (mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.LEFT_WRIST),\n","                    (mp_pose.PoseLandmark.RIGHT_SHOULDER, mp_pose.PoseLandmark.RIGHT_WRIST),\n","                    (mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.LEFT_INDEX),\n","                    (mp_pose.PoseLandmark.RIGHT_WRIST, mp_pose.PoseLandmark.RIGHT_INDEX)]\n","\n","        # Draw points for knees, hips, and shoulders\n","        for landmark in [mp_pose.PoseLandmark.LEFT_INDEX, mp_pose.PoseLandmark.RIGHT_INDEX,\n","                        mp_pose.PoseLandmark.LEFT_WRIST, mp_pose.PoseLandmark.RIGHT_WRIST,\n","                            mp_pose.PoseLandmark.LEFT_HIP, mp_pose.PoseLandmark.RIGHT_HIP,\n","                            mp_pose.PoseLandmark.LEFT_SHOULDER, mp_pose.PoseLandmark.RIGHT_SHOULDER]:\n","                plt.scatter(results.pose_landmarks.landmark[landmark.value].x * image.shape[1],\n","                            results.pose_landmarks.landmark[landmark.value].y * image.shape[0],\n","                            color='white', s=100)\n","\n","        for connection in connections:\n","            start_point = connection[0]\n","            end_point = connection[1]\n","            plt.plot([landmarks[start_point.value].x * image.shape[1], landmarks[end_point.value].x * image.shape[1]],\n","                    [landmarks[start_point.value].y * image.shape[0], landmarks[end_point.value].y * image.shape[0]],\n","                    color='magenta', linestyle='-', linewidth=1)\n","        # Draw landmarks and connections\n","        mp_drawing = mp.solutions.drawing_utils\n","        mp_drawing.draw_landmarks(image_rgb, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n","                                landmark_drawing_spec=mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=5),\n","                                connection_drawing_spec=mp_drawing.DrawingSpec(color=(255, 255, 255), thickness=2))\n","\n","        plt.axis('off')\n","        # Save the plot as bytes in memory\n","        buffer = io.BytesIO()\n","        plt.savefig(buffer, format='png')\n","        buffer.seek(0)\n","        # Convert the plot bytes to base64 string\n","        plot_base64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n","\n","        plt.close()  # Close the plot to free memory\n","\n","        return plot_base64, error, is_raising\n","\n","    # Process the image and get the base64 encoded plot\n","    plot_base64, error, is_raising = process_image(image_path)\n","    error_text = \"{:.2f}\".format(error)\n","\n","\n","    if is_raising and error <= 5:\n","        return jsonify({'image_base64': plot_base64, 'error_description': \"وضعية الصلاة بالصوره المرفوعه وضعية استقامه مثاليه\"})\n","    if is_raising and error>5 and error<=15:\n","        return jsonify({'image_base64': plot_base64, 'error_description': \" استقامه صحيحه لكن فيه نسبة انحراف عن الأصل بمقدار:{} % للتأكد من الصفة الصحيحة يمكنك مراجعة ارشاداتنا\".format(error_text)})\n","    if is_raising and error>15 and error<=30:\n","        return jsonify({'image_base64': plot_base64, 'error_description': \" استقامه صحيح لكن فيه نسبة انحراف كبيرة عن الأصل بمقدار:{} % للتأكد من الصفة الصحيحة يمكنك مراجعة ارشاداتنا\".format(error_text)})\n","    else:\n","        return jsonify({'image_base64': plot_base64, 'error_description': \"وضعية الصلاه بالصوره المرفوعه ليست وضعية استقامه\"})\n","\n","\n","if __name__ == \"__main__\":\n","    app.run(port=5000)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3eiCuR4x4vsg"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sRUMpymPKq0n"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":0}